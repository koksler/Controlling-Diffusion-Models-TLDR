# GAN, Generative Adversarial Networks

Ганки строяться на соревновании двух моделей, **генеративной** и **дискриминативной**. 

Построили их так, потому что прогресс дискриминативных моделей (или классификационных, определяющих что находится на картинке) был намного быстрее из-за уже существующих и применяемых алгоритмов [[#^de1a73|backpropagation, dropout, ReLU]].

Генеративные модели при этом пытались **явно** представить **как должны распределяться данные**. На деле это требует настолько точных и при этом сложных вычислений (тут проблема в экстремальном росте размерности). 

Поэтому легче было связать дискриминационку и генеративку, чтобы модель пыталась достичь не математически точного изображения, а сделать **обманку**.

![[Pasted image 20250924144809.png | center]]

> [!info] Определение
> ### Генеративные Состязательные Сети
> Двухмодельная система, состоящая из **генератора** и **дискриминатора**. 
> 
> Задача первого - создать такой объект, который похож на объект из тренировочной выборки, но не состоит в ней. 
> 
> Дискриминатор должен определить, состоит ли объект в выборке результатов генератора или в тренировочной выборке. Тем самым модель строиться на том, чтобы **генератор в конечном итоге смог обмануть дискриминатор**.

> The generative model can be thought of as analogous to a team of counterfeiters, while the discriminative model is analogous to the police^[1]

Короче говоря так мы пришли к неявному критерию для оценки генераций. И из-за игры в обман генеративная модель итеративно подстраивается под дискриминатора, улучшая тем самым свои результаты.

![[Pasted image 20250924144929.png]]
[отседова](https://www.tensorflow.org/tutorials/generative/dcgan)

> [!tip]- ^de1a73
> ### Бекпропагация
> Обратное распространение ошибки. Алгоритм вычисления градиентов функции потерь.
> 
> 1. Нейронка проходит через объект (допустим картинка с собакой). Говорит, что это танк M1 Abrams.
> 2. Результат сравнивается с правильным ответом, вычисляется ошибка (расстояние от-до) M1 Abrams не особо похож на собаку, поэтому увы.
> 3. Вычисляется какие веса повлияли на ошибку (по правилу цепи). Другими словами градиент ошибки по каждому весу. Потом движемся по каждому антиградиенту, где надо
> 4. Веса обновляются, колесо сансары даёт оборот.
> ### Дропаут
> Рандомно убираем какой-то процент нейронов в сети, чтобы сеть не переобучалась и не зависила только от какой-нибудь пачки нейронов.
> ### ReLU, Rectified Linear Unit
> **Если вход $x$ больше нуля**, то выход функции равен **$x$**
> **Если вход меньше или равен нулю**, то выход функции равен **$0$**
> 
> На очень умном: $f(x) = \max(0, x)$
> 
> Эта херня быстро вычисляется, обнуляет отрицательные значения. Но иногда когда вход равен 0, нейрон может отключиться насовсем.
## Схуяли мы от них отказались

GAN в своей основе не может быть надежной.

Дискриминатор может быстро обогнать генератор, ибо он изначально в лучшем положении, так как генератор по началу в любом случае будет генерировать очевидные фейки. Уверенность дикриминатора при этом близка к 100%.

Функция потерь в классическом GAN это
$$\log{(1-D(G(z)))}$$
$G(z)$ результат генератора. Функция $D$ это уверенность. Если дискриминатор слишком самоуверенный то $D(G(z))$ будет близок к 0. А $\log{(1)}$ это очень плоская функция
![[Pasted image 20250924165827.png | center]]
А значит результат функции будет почти нулевым. И из-за этого **отмирает бекпропагация**, так как веса будут обновляться очень незначительно. Называется это Vanishing Gradients.

> [!info] Определение
> ### Vanishing Gradients
> Проблема, когда дискриминатор становится слишком сильным и генератор перестает обучаться.

Может быть и обратная ситуация. Генератор находит дыру в дискриминаторе и начинает фокусироваться на ней. Он может сгенерировать что-то очень крутое, и продолжать генерировать мелкие вариации этого же изображение (потому что оно работает, зачем что-то менять). 

Цель свою генератор выполняет, до того как дискриминатор начнёт видеть закономерности. Сводиться это к тому, что генератор не может изменить свои генерации, становиться однообразным и больше неэффективным. Это Mode Collapse

> [!info] Определение
> ### Mode Collapse
> Генератор находит одну-две удачные картинки и начинает генерировать только их, игнорируя разнообразие.

А ещё обучение само по себе нестабильно, ибо генератор руководствуется только фидбеком от дискриминатора. Две модели тянут канат друг на друга, из-за этого их функция потерь колеблеться вверх-вниз, не находя равновесия. При этом колебания лоссов непредсказуемые, потому что нельзя точно сказать когда генератор обманет дискриминатор. Изменения в процессе обучения вводят ещё больше нестабильности. Це Training Instability

> [!info] Определение
> ### Training Instability
> Между генератором и дискриминатором идёт постоянное перекачивание, из-за чего их лоссы постоянно колеблются. Это нестабильно и фу

**В диффузионных моделях нет войны дискриминатора и генератора.** Они реставрируют шум, учась на зашумлении уже существующих картинок. Проблем GAN тут быть не может, потому что принцип совсем другой.

****
[[02_Диффузионные_модели]]

[1]: [Generative Adversarial Networks 2014](https://arxiv.org/abs/1406.2661)